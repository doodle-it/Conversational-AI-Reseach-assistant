{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from wordfreq import get_frequency_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pdfplumber pdfminer -q\n",
    "\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_pdf_to_text(path):\n",
    "    \"\"\"Convert the PDF to text and store it in the self.content\n",
    "    attribute.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    pdf = pdfplumber.open(path)\n",
    "    for page in range(len(pdf.pages)):\n",
    "\n",
    "        page_obj = pdf.pages[page]\n",
    "\n",
    "        text.append(\n",
    "            page_obj.extract_text(\n",
    "                x_tolerance=1, x_tolerance_ratio=None, y_tolerance=1, layout=False, x_density=7.25, y_density=13, line_dir_render=None,keep_blank_chars=True,use_text_flow=True, char_dir_render=None, expand_ligatures=True\n",
    "            )\n",
    "        )\n",
    "    text = \"\\n\".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Raw_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Raw_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 251\u001b[0m\n\u001b[1;32m    248\u001b[0m pdf_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./../Createdb/docs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pdf_directory):\n\u001b[0;32m--> 251\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m process_directory(pdf_directory)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# Check if the combined DataFrame is not empty\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m combined_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;66;03m# Export the combined DataFrame to CSV\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[96], line 239\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    238\u001b[0m         pdf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[0;32m--> 239\u001b[0m         df \u001b[38;5;241m=\u001b[39m process_pdf(pdf_path)  \n\u001b[1;32m    240\u001b[0m         combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([combined_df, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dirs:\n",
      "Cell \u001b[0;32mIn[96], line 214\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clean\n\u001b[1;32m    213\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(split_message)\n\u001b[0;32m--> 214\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRaw_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(raw)\n\u001b[1;32m    215\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(header)\n\u001b[1;32m    216\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(abstract)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Raw_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        text =  _convert_pdf_to_text(file)\n",
    "\n",
    "        \n",
    "\n",
    "        def pa(text):\n",
    "            pattern_abstract = r'\\s*a\\s*b\\s*s\\s*t\\s*r\\s*a\\s*c\\s*t\\s*'\n",
    "            matches1 = re.findall(pattern_abstract, text)\n",
    "            if matches1:\n",
    "                match = ''.join(matches1[0])\n",
    "                pattern_abstract = re.compile(pattern_abstract)\n",
    "                return pattern_abstract.sub('\\nabstract\\n', text)\n",
    "            else:\n",
    "                return text\n",
    "\n",
    "        def pi(text):\n",
    "            pattern_intro = r'i\\s*n\\s*t\\s*r\\s*o\\s*d\\s*u\\s*c\\s*t\\s*i\\s*o\\s*n'\n",
    "            matches1 = re.findall(pattern_intro, text)\n",
    "            if matches1:\n",
    "                match = ''.join(matches1[0])\n",
    "                pattern_intro = re.compile(pattern_intro)\n",
    "                return pattern_intro.sub('\\nintroduction\\n', text)\n",
    "            else:\n",
    "                return text\n",
    "\n",
    "        def pr(text):\n",
    "            pattern_intro = r'r\\s*e\\s*f\\s*e\\s*r\\s*e\\s*n\\s*c\\s*e\\s*s'\n",
    "            matches1 = re.findall(pattern_intro, text)\n",
    "            if matches1:\n",
    "                match = ''.join(matches1[0])\n",
    "                pattern_intro = re.compile(pattern_intro)\n",
    "                return pattern_intro.sub('\\nreferences\\n', text)\n",
    "            else:\n",
    "                return text\n",
    "            \n",
    "        text = pa(text)\n",
    "        text = pi(text)\n",
    "        text = pr(text)\n",
    "        # print(text.lower())\n",
    "        text = [[text]]\n",
    "        df = pd.DataFrame(text, columns=['raw_text'])\n",
    "\n",
    "        def find_text_by_largest_font(page):\n",
    "            largest_font_size = 0\n",
    "            largest_font_chars = []\n",
    "\n",
    "            for char in page.chars:\n",
    "                size = char[\"size\"]\n",
    "                if size > largest_font_size:\n",
    "                    largest_font_size = size\n",
    "                    largest_font_chars = [char]\n",
    "                elif size == largest_font_size:\n",
    "                    largest_font_chars.append(char)\n",
    "\n",
    "            return largest_font_chars\n",
    "\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            largest_font_chars = find_text_by_largest_font(first_page)\n",
    "            largest_font_text = ''.join(char['text'] for char in largest_font_chars)\n",
    "        df['Title'] = largest_font_text\n",
    "\n",
    "\n",
    "        word_prob = get_frequency_dict(lang='en', wordlist='large')\n",
    "        max_word_len = max(map(len, word_prob))  # 34\n",
    "\n",
    "        # def str_slice(text):\n",
    "        def viterbi_segment(text, debug=False):\n",
    "            probs, lasts = [1.0], [0]\n",
    "            for i in range(1, len(text) + 1):\n",
    "                new_probs = []\n",
    "                for j in range(max(0, i - max_word_len), i):\n",
    "                    substring = text[j:i]\n",
    "                    length_reward = np.exp(len(substring))\n",
    "                    freq = word_prob.get(substring, 0) * length_reward\n",
    "                    compounded_prob = probs[j] * freq\n",
    "                    new_probs.append((compounded_prob, j))\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f'[{j}:{i}] = \"{text[lasts[j]:j]} & {substring}\" = ({probs[j]:.8f} & {freq:.8f}) = {compounded_prob:.8f}')\n",
    "\n",
    "                prob_k, k = max(new_probs)  # max of a touple is the max across the first elements, which is the max of the compounded probabilities\n",
    "                probs.append(prob_k)\n",
    "                lasts.append(k)\n",
    "\n",
    "                if debug:\n",
    "                    print(f'i = {i}, prob_k = {prob_k:.8f}, k = {k}, ({text[k:i]})\\n')\n",
    "\n",
    "\n",
    "            # when text is a word that doesn't exist, the algorithm breaks it into individual letters.\n",
    "            # in that case, return the original word instead\n",
    "            if len(set(lasts)) == len(text):\n",
    "                return text\n",
    "\n",
    "            words = []\n",
    "            k = len(text)\n",
    "            while 0 < k:\n",
    "                word = text[lasts[k]:k]\n",
    "                words.append(word)\n",
    "                k = lasts[k]\n",
    "            words.reverse()\n",
    "            return ' '.join(words)\n",
    "\n",
    "        def split_message(message):\n",
    "            new_message = ' '.join(viterbi_segment(wordmash, debug=False) for wordmash in message.split())\n",
    "            return new_message\n",
    "\n",
    "        def raw(text):\n",
    "            if 'introduction' in text:\n",
    "                new = text.split('introduction')\n",
    "                return new[1] \n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        def header(text):\n",
    "            if 'introduction' in text:\n",
    "                new = text.split('introduction')\n",
    "                return new[0]   \n",
    "            else:\n",
    "                return ''\n",
    "            \n",
    "        def abstract(text):\n",
    "            new = text.split('abstract')\n",
    "            if len(new) > 1:\n",
    "                return new[1]\n",
    "            else:\n",
    "                return ''  \n",
    "            \n",
    "            \n",
    "\n",
    "        def pc(data_text):\n",
    "            data_text = data_text.split('\\\"' and \",\" and \":\" and '\\”' and '\\“')\n",
    "            text = \"\"\n",
    "            for i in data_text:\n",
    "                text = text + i\n",
    "            print(len(text))\n",
    "\n",
    "            text = \"\"\n",
    "\n",
    "            for i in data_text:\n",
    "                text = text + i\n",
    "    \n",
    "            return text\n",
    "        \n",
    "        mail_pattern = re.compile(r'\\b\\S+@\\S+\\.\\S+\\.?\\S*\\b')\n",
    "        def remove_mail(text):\n",
    "            return mail_pattern.sub('', text)\n",
    "        \n",
    "        url_pattern = r'https?://(?:www\\.)?[^ ]+|www[^ ]+'\n",
    "\n",
    "        def add_urls(text):\n",
    "            httplinks = re.findall(url_pattern,text)\n",
    "            return httplinks\n",
    "\n",
    "        url_pattern = re.compile(url_pattern)\n",
    "        \n",
    "        def remove_urls(text):\n",
    "            return url_pattern.sub('', text)\n",
    "        \n",
    "        def ref(text):\n",
    "            if('references' in text):\n",
    "                terms = text.split('references')\n",
    "                return terms[1]\n",
    "            else:\n",
    "                return ''\n",
    "        def wref(text):\n",
    "            if('references' in text):\n",
    "                terms = text.split('references')\n",
    "                return terms[0]\n",
    "            else:\n",
    "                return text\n",
    "            \n",
    "        number_pattern = re.compile(r'\\b(?<!\\[)(-?\\d+(?:\\.\\d+)?)\\b(?![^\\[]*\\])')\n",
    "\n",
    "        def remove_number(text):\n",
    "            return number_pattern.sub('', text)\n",
    "\n",
    "\n",
    "        def sen_seg(data_text):\n",
    "            sentences = []\n",
    "\n",
    "            sentence = ''\n",
    "\n",
    "            # Iterate through the text\n",
    "            for i in range(len(data_text)):\n",
    "                char = data_text[i]\n",
    "                # Append character to the current sentence\n",
    "                sentence += char\n",
    "                if char == '.':\n",
    "                    # Check if the character before and after '.' are both numbers\n",
    "                    if i > 0 and i < len(data_text) - 1 and data_text[i-1].isdigit() and data_text[i+1].isdigit():\n",
    "                        continue  # Skip adding '.' to the sentence\n",
    "                    else:\n",
    "                        sentences.append(sentence.strip())\n",
    "                        sentence = ''\n",
    "\n",
    "            return sentences\n",
    "        \n",
    "        def checker(sentences):\n",
    "            new = []\n",
    "            for s in sentences:\n",
    "                if(len(s) > 30):\n",
    "                    new.append(s)\n",
    "            return new\n",
    "\n",
    "        \n",
    "        def merge_sen(sentences):\n",
    "            separator = \" \"\n",
    "            clean = separator.join(sentences)\n",
    "            return clean\n",
    "\n",
    "        df['Title'] = df['Title'].apply(split_message)\n",
    "        df['raw_text'] = df['raw_text'].apply(raw)\n",
    "        df['header'] = df['raw_text'].apply(header)\n",
    "        df['abstract'] = df['header'].apply(abstract)\n",
    "        df['pc_removal'] = df['raw_text'].apply(pc)\n",
    "        df['wmail'] = df['pc_removal'].apply(remove_mail)\n",
    "        df['urls'] = df['wmail'].apply(add_urls)\n",
    "        df['wurl'] = df['wmail'].apply(remove_urls)\n",
    "        df['refs'] = df['wurl'].apply(ref)\n",
    "        df['wrefs'] = df['wurl'].apply(wref)\n",
    "        df['wnumbers'] = df['wrefs']\n",
    "        df['sentences'] = df['wnumbers'].apply(sen_seg)\n",
    "        df['sentences'] = df['sentences'].apply(checker)\n",
    "        df['clean'] = df['sentences'].apply(merge_sen)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    combined_df = pd.DataFrame() \n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                df = process_pdf(pdf_path)  \n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "        for dir in dirs:\n",
    "            subdir = os.path.join(root, dir)\n",
    "            process_directory(subdir)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "pdf_directory = './../Createdb/docs'\n",
    "\n",
    "if os.path.isdir(pdf_directory):\n",
    "    combined_df = process_directory(pdf_directory)\n",
    "    # Check if the combined DataFrame is not empty\n",
    "    if not combined_df.empty:\n",
    "        # Export the combined DataFrame to CSV\n",
    "        combined_df.to_csv(\"combined_data.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Combined DataFrame is empty.\")\n",
    "else:\n",
    "    if pdf_directory.endswith('.pdf'):\n",
    "        df = process_pdf(pdf_directory)  \n",
    "        # Check if the DataFrame is not empty\n",
    "        if not df.empty:\n",
    "            # Export the DataFrame to CSV\n",
    "            df.to_csv(\"combined_data.csv\", index=False)\n",
    "        else:\n",
    "            print(\"DataFrame is empty.\")\n",
    "    else:\n",
    "        print(\"Invalid path. Please provide a directory or a PDF file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replacing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\na b s t r a c t\\n']\n",
      "\n",
      "a b s t r a c t\n",
      "\n",
      "['introduction']\n",
      "introduction\n",
      "['references']\n",
      "references\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def pa(text):\n",
    "    # text = \"abstract  abstract  abstract\"\n",
    "    pattern_abstract = r'\\s*a\\s*b\\s*s\\s*t\\s*r\\s*a\\s*c\\s*t\\s*'\n",
    "    matches1 = re.findall(pattern_abstract,text)\n",
    "    print(matches1)\n",
    "    match = ''.join(matches1[0])\n",
    "    print(match)\n",
    "    pattern_abstract = re.compile(pattern_abstract)\n",
    "    return pattern_abstract.sub('\\nabstract\\n',text)\n",
    "    # matches1 = re.findall(pattern_abstract, text)\n",
    "    # text.replace(match,'abstract')\n",
    "def pi(text):\n",
    "    # text = \"abstract  abstract  abstract\"\n",
    "    pattern_intro = r'i\\s*n\\s*t\\s*r\\s*o\\s*d\\s*u\\s*c\\s*t\\s*i\\s*o\\s*n'\n",
    "    matches1 = re.findall(pattern_intro,text)\n",
    "    print(matches1)\n",
    "    match = ''.join(matches1[0])\n",
    "    print(match)\n",
    "    pattern_intro = re.compile(pattern_intro)\n",
    "    return pattern_intro.sub('\\nintroduction\\n',text)\n",
    "    # matches1 = re.findall(pattern_abstract, text)\n",
    "    # text.replace(match,'abstract')\n",
    "def pr(text):\n",
    "    # text = \"abstract  abstract  abstract\"\n",
    "    pattern_intro = r'r\\s*e\\s*f\\s*e\\s*r\\s*e\\s*n\\s*c\\s*e\\s*s'\n",
    "    matches1 = re.findall(pattern_intro,text)\n",
    "    if len(matches1) == 0:\n",
    "        return text \n",
    "    print(matches1)\n",
    "    match = ''.join(matches1[0])\n",
    "    print(match)\n",
    "    pattern_intro = re.compile(pattern_intro)\n",
    "    return pattern_intro.sub('\\nreferences\\n',text)\n",
    "    # matches1 = re.findall(pattern_abstract, text)\n",
    "    # text.replace(match,'abstract')\n",
    "text = pa(text)\n",
    "text = pi(text)\n",
    "text = pr(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[text]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['raw_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding title from first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may not be accurate \n",
    "# we are assuming that the text with the largest font is the title\n",
    "\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "def find_text_by_largest_font(page):\n",
    "    largest_font_size = 0\n",
    "    largest_font_chars = []\n",
    "\n",
    "    for char in page.chars:\n",
    "        size = char[\"size\"]\n",
    "        if size > largest_font_size:\n",
    "            largest_font_size = size\n",
    "            largest_font_chars = [char]\n",
    "        elif size == largest_font_size:\n",
    "            largest_font_chars.append(char)\n",
    "    # for char in page.chars:\n",
    "    #     size = char[\"size\"]\n",
    "    #     if size == largest_font_size or char == \" \":\n",
    "    #         largest_font_chars.append(char)\n",
    "\n",
    "    return largest_font_chars\n",
    "\n",
    "\n",
    "with pdfplumber.open(path) as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    largest_font_chars = find_text_by_largest_font(first_page)\n",
    "\n",
    "    largest_font_text = ''.join(char['text'] for char in largest_font_chars)\n",
    "\n",
    "    # print(\"-----\")\n",
    "    # print(largest_font_text)\n",
    "    # print(\"-----\")\n",
    "df['Title'] = largest_font_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>journal of constructional steel research 65 (2...</td>\n",
       "      <td>JournalofConstructionalSteelResearch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  journal of constructional steel research 65 (2...   \n",
       "\n",
       "                                  Title  \n",
       "0  JournalofConstructionalSteelResearch  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Slicing for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordfreq in /Users/hritik/anaconda3/lib/python3.11/site-packages (3.1.1)\n",
      "Requirement already satisfied: ftfy>=6.1 in /Users/hritik/anaconda3/lib/python3.11/site-packages (from wordfreq) (6.2.0)\n",
      "Requirement already satisfied: langcodes>=3.0 in /Users/hritik/anaconda3/lib/python3.11/site-packages (from wordfreq) (3.3.0)\n",
      "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /Users/hritik/anaconda3/lib/python3.11/site-packages (from wordfreq) (1.1.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /Users/hritik/anaconda3/lib/python3.11/site-packages (from wordfreq) (1.0.8)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /Users/hritik/anaconda3/lib/python3.11/site-packages (from wordfreq) (2023.12.25)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/hritik/anaconda3/lib/python3.11/site-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may or may not work depending on the dictionary\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from wordfreq import get_frequency_dict\n",
    "\n",
    "word_prob = get_frequency_dict(lang='en', wordlist='large')\n",
    "max_word_len = max(map(len, word_prob))  # 34\n",
    "\n",
    "# def str_slice(text):\n",
    "def viterbi_segment(text, debug=False):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        new_probs = []\n",
    "        for j in range(max(0, i - max_word_len), i):\n",
    "            substring = text[j:i]\n",
    "            length_reward = np.exp(len(substring))\n",
    "            freq = word_prob.get(substring, 0) * length_reward\n",
    "            compounded_prob = probs[j] * freq\n",
    "            new_probs.append((compounded_prob, j))\n",
    "            \n",
    "            if debug:\n",
    "                print(f'[{j}:{i}] = \"{text[lasts[j]:j]} & {substring}\" = ({probs[j]:.8f} & {freq:.8f}) = {compounded_prob:.8f}')\n",
    "\n",
    "        prob_k, k = max(new_probs)  # max of a touple is the max across the first elements, which is the max of the compounded probabilities\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "\n",
    "        if debug:\n",
    "            print(f'i = {i}, prob_k = {prob_k:.8f}, k = {k}, ({text[k:i]})\\n')\n",
    "\n",
    "\n",
    "    # when text is a word that doesn't exist, the algorithm breaks it into individual letters.\n",
    "    # in that case, return the original word instead\n",
    "    if len(set(lasts)) == len(text):\n",
    "        return text\n",
    "\n",
    "    words = []\n",
    "    k = len(text)\n",
    "    while 0 < k:\n",
    "        word = text[lasts[k]:k]\n",
    "        words.append(word)\n",
    "        k = lasts[k]\n",
    "    words.reverse()\n",
    "    return ' '.join(words)\n",
    "\n",
    "def split_message(message):\n",
    "    new_message = ' '.join(viterbi_segment(wordmash, debug=False) for wordmash in message.split())\n",
    "    return new_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw(text):\n",
    "    if 'introduction' in text:\n",
    "        new = text.split('introduction')\n",
    "        return new[1] \n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def header(text):\n",
    "    if 'introduction' in text:\n",
    "        new = text.split('introduction')\n",
    "        return new[0]   \n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def abstract(text):\n",
    "\n",
    "    new = text.split('abstract')\n",
    "    return new[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re \n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "def pc(data_text):\n",
    "    data_text = data_text.split('\\\"' and \",\" and \":\" and '\\”' and '\\“')\n",
    "    text = \"\"\n",
    "    for i in data_text:\n",
    "        text = text + i\n",
    "    print(len(text))\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    for i in data_text:\n",
    "        text = text + i\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mail\n",
    "mail_pattern = re.compile(r'\\b\\S+@\\S+\\.\\S+\\.?\\S*\\b')\n",
    "def remove_mail(text):\n",
    "    return mail_pattern.sub('', text)\n",
    "\n",
    "# df['without_mail'] = df['without_refs'].apply(remove_mail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #url\n",
    "url_pattern = r'https?://(?:www\\.)?[^ ]+|www[^ ]+'\n",
    "def add_urls(text):\n",
    "    httplinks = re.findall(url_pattern,text)\n",
    "    return httplinks\n",
    "\n",
    "url_pattern = re.compile(url_pattern)\n",
    "def remove_urls(text):\n",
    "    return url_pattern.sub('', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['urls'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split refrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ref(text):\n",
    "    if('references' in text):\n",
    "        terms = text.split('references')\n",
    "        return terms[1]\n",
    "    else:\n",
    "        return ''\n",
    "def wref(text):\n",
    "    if('references' in text):\n",
    "        terms = text.split('references')\n",
    "        return terms[0]\n",
    "    else:\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df['refs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "number_pattern = re.compile(r'\\b(?<!\\[)(-?\\d+(?:\\.\\d+)?)\\b(?![^\\[]*\\])')\n",
    "\n",
    "def remove_number(text):\n",
    "    return number_pattern.sub('', text)\n",
    "\n",
    "# Assuming df['without_url'] is your DataFrame column containing text\n",
    "# df['wnumbers'] = df['wrefs'].apply(remove_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_seg(data_text):\n",
    "    sentences = []\n",
    "\n",
    "    sentence = ''\n",
    "\n",
    "    # Iterate through the text\n",
    "    for i in range(len(data_text)):\n",
    "        char = data_text[i]\n",
    "        # Append character to the current sentence\n",
    "        sentence += char\n",
    "        if char == '.':\n",
    "            # Check if the character before and after '.' are both numbers\n",
    "            if i > 0 and i < len(data_text) - 1 and data_text[i-1].isdigit() and data_text[i+1].isdigit():\n",
    "                continue  # Skip adding '.' to the sentence\n",
    "            else:\n",
    "                sentences.append(sentence.strip())\n",
    "                sentence = ''\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Assuming df['wnumbers'] contains the text data\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df['wnumbers'] contains the text data\n",
    "# sen_seg('fjdkfj.fdsfds.s 3.4 dfds.dsa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(df['sentences'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(sentences):\n",
    "    new = []\n",
    "    for s in sentences:\n",
    "        if(len(s) > 30):\n",
    "            new.append(s)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "\n",
    "# selecting sentences with more characters than 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sen(sentences):\n",
    "    separator = \" \"\n",
    "    clean = separator.join(sentences)\n",
    "    return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36210\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df['Title'] = df['Title'].apply(split_message)\n",
    "df['header'] = df['raw_text'].apply(header)\n",
    "df['raw_text'] = df['raw_text'].apply(raw)\n",
    "df['abstract'] = df['header'].apply(abstract)\n",
    "df['pc_removal'] = df['raw_text'].apply(pc)\n",
    "df['wmail'] = df['pc_removal'].apply(remove_mail)\n",
    "df['urls'] = df['wmail'].apply(add_urls)\n",
    "df['wurl'] = df['wmail'].apply(remove_urls)\n",
    "df['refs'] = df['wurl'].apply(ref)\n",
    "df['wrefs'] = df['wurl'].apply(wref)\n",
    "df['wnumbers'] = df['wrefs']\n",
    "df['sentences'] = df['wnumbers'].apply(sen_seg)\n",
    "df['sentences'] = df['sentences'].apply(checker)\n",
    "df['clean'] = df['sentences'].apply(merge_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preProcessed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
