{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from wordfreq import get_frequency_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pdfplumber pdfminer -q\n",
    "\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_pdf_to_text(path):\n",
    "    \"\"\"Convert the PDF to text and store it in the self.content\n",
    "    attribute.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    pdf = pdfplumber.open(path)\n",
    "    for page in range(len(pdf.pages)):\n",
    "\n",
    "        page_obj = pdf.pages[page]\n",
    "\n",
    "        text.append(\n",
    "            page_obj.extract_text(\n",
    "                x_tolerance=1, x_tolerance_ratio=None, y_tolerance=1, layout=False, x_density=7.25, y_density=13, line_dir_render=None,keep_blank_chars=True,use_text_flow=True, char_dir_render=None, expand_ligatures=True\n",
    "            )\n",
    "        )\n",
    "    text = \"\\n\".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ammtiac.alionscience.com the ammtiac quarterly, volume 3, number 4 3\n",
      "\n",
      "http://ammtiac.alionscience.com the ammtiac quarterly, volume 3, number 4 3\n",
      "\n",
      "23667\n",
      "structural damage identification in laminated structures\n",
      "using frf data\n",
      "j.v. arau´ jo dos santos, c.m. mota soares *, c.a. mota soares, n.m.m. maia\n",
      "idmec/ist, instituto superior te´cnico, av. rovisco pais, 1049-001 lisbon, portugal\n",
      "available online 5 november 2004\n",
      "abstract\n",
      "a damage identification technique based on frequency response functions (frf) sensitivities is presented. this technique leads to\n",
      "a set of linear equations, which is solved using an algorithm that constrains the solution to be physically admissible. damage sim-\n",
      "ulation and identification on a laminated rectangular plate is performed. the influence of the number of natural frequencies and\n",
      "mode shapes used on the frf computation, as well as the frequency range, the excitation location and the number of measured\n",
      "degrees of freedom (m-dof) is studied. numerical tests show that the best accuracy is obtained when using the dynamic expansion\n",
      "of the m-dof. it is also demonstrated that for small damage the errors are the main influence, whereas for large damage the model\n",
      "incompleteness becomes the most important factor in the results. a procedure for weighting and deletion of equations is used to\n",
      "obtain better identification results. the results of the technique presented in this paper versus those obtained by a technique based\n",
      "on modal data is also discussed.\n",
      "(cid:1) 2004 elsevier ltd. all rights reserved.\n",
      "keywords: damage identification; frequency response functions; model incompleteness; measurement noise and error; static and dynamic expa-\n",
      "nsions; laminated plate\n",
      "1. \n",
      "structural damage identification in laminated structures\n",
      "using frf data\n",
      "j.v. arau´ jo dos santos, c.m. mota soares *, c.a. mota soares, n.m.m. maia\n",
      "idmec/ist, instituto superior te´cnico, av. rovisco pais, 1049-001 lisbon, portugal\n",
      "available online 5 november 2004\n",
      "abstract\n",
      "a damage identification technique based on frequency response functions (frf) sensitivities is presented. this technique leads to\n",
      "a set of linear equations, which is solved using an algorithm that constrains the solution to be physically admissible. damage sim-\n",
      "ulation and identification on a laminated rectangular plate is performed. the influence of the number of natural frequencies and\n",
      "mode shapes used on the frf computation, as well as the frequency range, the excitation location and the number of measured\n",
      "degrees of freedom (m-dof) is studied. numerical tests show that the best accuracy is obtained when using the dynamic expansion\n",
      "of the m-dof. it is also demonstrated that for small damage the errors are the main influence, whereas for large damage the model\n",
      "incompleteness becomes the most important factor in the results. a procedure for weighting and deletion of equations is used to\n",
      "obtain better identification results. the results of the technique presented in this paper versus those obtained by a technique based\n",
      "on modal data is also discussed.\n",
      "(cid:1) 2004 elsevier ltd. all rights reserved.\n",
      "keywords: damage identification; frequency response functions; model incompleteness; measurement noise and error; static and dynamic expa-\n",
      "nsions; laminated plate\n",
      "1. \n",
      "35666\n",
      "journal of constructional steel research 65 (2009) 1558–1568\n",
      "contents lists available at sciencedirect\n",
      "journal of constructional steel research\n",
      "journal homepage: www.elsevier.com/locate/jcsr\n",
      "a particle swarm ant colony optimization for truss structures with\n",
      "discrete variables\n",
      "a. kaveh\n",
      "a,∗\n",
      ",1\n",
      ", s. talatahari\n",
      "b\n",
      "a\n",
      "institute for mechanics of materials and structures, vienna university of technology, karlsplatz 13, a-1040 wien, austria\n",
      "b\n",
      "department of civil engineering, tabriz university, tabriz, iran\n",
      "a r t i c l e i n f o\n",
      "article history:\n",
      "received 19 march 2009\n",
      "accepted 24 april 2009\n",
      "keywords:\n",
      "heuristic particle swarm ant colony\n",
      "optimization\n",
      "harmony search\n",
      "optimum truss design\n",
      "discrete variables\n",
      "abstract\n",
      "in this paper, a particle swarm optimizer with passive congregation (psopc), ant colony optimization\n",
      "(aco) and harmony search scheme (hs) are combined to reach to an efficient algorithm, called discrete\n",
      "heuristic particle swarm ant colony optimization (dhpsaco). this method is then employed to optimize\n",
      "truss structures with discrete variables. the dhpsaco applies a psopc for global optimization and the ant\n",
      "colony approach for local search, similar to its continuous version. the problem-specific constraints are\n",
      "handled using a modified feasible-based mechanism, and the harmony search scheme is employed to deal\n",
      "with variable constraints. some design examples are tested using the new method and their results are\n",
      "compared to those of pso, psopc and hpso algorithms to demonstrate the effectiveness of the present\n",
      "method.\n",
      "©\n",
      "2009 elsevier ltd. all rights reserved.\n",
      "1. \n",
      "journal of constructional steel research 65 (2009) 1558–1568\n",
      "contents lists available at sciencedirect\n",
      "journal of constructional steel research\n",
      "journal homepage: www.elsevier.com/locate/jcsr\n",
      "a particle swarm ant colony optimization for truss structures with\n",
      "discrete variables\n",
      "a. kaveh\n",
      "a,∗\n",
      ",1\n",
      ", s. talatahari\n",
      "b\n",
      "a\n",
      "institute for mechanics of materials and structures, vienna university of technology, karlsplatz 13, a-1040 wien, austria\n",
      "b\n",
      "department of civil engineering, tabriz university, tabriz, iran\n",
      "a r t i c l e i n f o\n",
      "article history:\n",
      "received 19 march 2009\n",
      "accepted 24 april 2009\n",
      "keywords:\n",
      "heuristic particle swarm ant colony\n",
      "optimization\n",
      "harmony search\n",
      "optimum truss design\n",
      "discrete variables\n",
      "abstract\n",
      "in this paper, a particle swarm optimizer with passive congregation (psopc), ant colony optimization\n",
      "(aco) and harmony search scheme (hs) are combined to reach to an efficient algorithm, called discrete\n",
      "heuristic particle swarm ant colony optimization (dhpsaco). this method is then employed to optimize\n",
      "truss structures with discrete variables. the dhpsaco applies a psopc for global optimization and the ant\n",
      "colony approach for local search, similar to its continuous version. the problem-specific constraints are\n",
      "handled using a modified feasible-based mechanism, and the harmony search scheme is employed to deal\n",
      "with variable constraints. some design examples are tested using the new method and their results are\n",
      "compared to those of pso, psopc and hpso algorithms to demonstrate the effectiveness of the present\n",
      "method.\n",
      "©\n",
      "2009 elsevier ltd. all rights reserved.\n",
      "1. \n",
      "36207\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        text =  _convert_pdf_to_text(file)\n",
    "\n",
    "        \n",
    "\n",
    "        def pa(text):\n",
    "            pattern_abstract = r'\\s*a\\s*b\\s*s\\s*t\\s*r\\s*a\\s*c\\s*t\\s*'\n",
    "            matches1 = re.findall(pattern_abstract, text)\n",
    "            if matches1:\n",
    "                match = ''.join(matches1[0])\n",
    "                pattern_abstract = re.compile(pattern_abstract)\n",
    "                return pattern_abstract.sub('\\nabstract\\n', text)\n",
    "            else:\n",
    "                return text\n",
    "\n",
    "        def pi(text):\n",
    "            pattern_intro = r'i\\s*n\\s*t\\s*r\\s*o\\s*d\\s*u\\s*c\\s*t\\s*i\\s*o\\s*n'\n",
    "            matches1 = re.findall(pattern_intro, text)\n",
    "            if matches1:\n",
    "                match = ''.join(matches1[0])\n",
    "                pattern_intro = re.compile(pattern_intro)\n",
    "                return pattern_intro.sub('\\nintroduction\\n', text)\n",
    "            else:\n",
    "                return text\n",
    "\n",
    "        def pr(text):\n",
    "            pattern_intro = r'r\\s*e\\s*f\\s*e\\s*r\\s*e\\s*n\\s*c\\s*e\\s*s'\n",
    "            matches1 = re.findall(pattern_intro, text)\n",
    "            if matches1:\n",
    "                match = ''.join(matches1[0])\n",
    "                pattern_intro = re.compile(pattern_intro)\n",
    "                return pattern_intro.sub('\\nreferences\\n', text)\n",
    "            else:\n",
    "                return text\n",
    "            \n",
    "        text = pa(text)\n",
    "        text = pi(text)\n",
    "        text = pr(text)\n",
    "        text = text.lower()\n",
    "        text = [[text]]\n",
    "        # print(text)\n",
    "        df = pd.DataFrame(text, columns=['raw_text'])\n",
    "        # print(df['raw_text'][0])\n",
    "        # df.head()\n",
    "        text = text[0]\n",
    "\n",
    "        def find_text_by_largest_font(page):\n",
    "            largest_font_size = 0\n",
    "            largest_font_chars = []\n",
    "\n",
    "            for char in page.chars:\n",
    "                size = char[\"size\"]\n",
    "                if size > largest_font_size:\n",
    "                    largest_font_size = size\n",
    "                    largest_font_chars = [char]\n",
    "                elif size == largest_font_size:\n",
    "                    largest_font_chars.append(char)\n",
    "\n",
    "            return largest_font_chars\n",
    "\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            largest_font_chars = find_text_by_largest_font(first_page)\n",
    "            largest_font_text = ''.join(char['text'] for char in largest_font_chars)\n",
    "        df['Title'] = largest_font_text\n",
    "\n",
    "\n",
    "        word_prob = get_frequency_dict(lang='en', wordlist='large')\n",
    "        max_word_len = max(map(len, word_prob))  # 34\n",
    "\n",
    "        # def str_slice(text):\n",
    "        def viterbi_segment(text, debug=False):\n",
    "            probs, lasts = [1.0], [0]\n",
    "            for i in range(1, len(text) + 1):\n",
    "                new_probs = []\n",
    "                for j in range(max(0, i - max_word_len), i):\n",
    "                    substring = text[j:i]\n",
    "                    length_reward = np.exp(len(substring))\n",
    "                    freq = word_prob.get(substring, 0) * length_reward\n",
    "                    compounded_prob = probs[j] * freq\n",
    "                    new_probs.append((compounded_prob, j))\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f'[{j}:{i}] = \"{text[lasts[j]:j]} & {substring}\" = ({probs[j]:.8f} & {freq:.8f}) = {compounded_prob:.8f}')\n",
    "\n",
    "                prob_k, k = max(new_probs)  # max of a touple is the max across the first elements, which is the max of the compounded probabilities\n",
    "                probs.append(prob_k)\n",
    "                lasts.append(k)\n",
    "\n",
    "                if debug:\n",
    "                    print(f'i = {i}, prob_k = {prob_k:.8f}, k = {k}, ({text[k:i]})\\n')\n",
    "\n",
    "\n",
    "            # when text is a word that doesn't exist, the algorithm breaks it into individual letters.\n",
    "            # in that case, return the original word instead\n",
    "            if len(set(lasts)) == len(text):\n",
    "                return text\n",
    "\n",
    "            words = []\n",
    "            k = len(text)\n",
    "            while 0 < k:\n",
    "                word = text[lasts[k]:k]\n",
    "                words.append(word)\n",
    "                k = lasts[k]\n",
    "            words.reverse()\n",
    "            return ' '.join(words)\n",
    "\n",
    "        def split_message(message):\n",
    "            new_message = ' '.join(viterbi_segment(wordmash, debug=False) for wordmash in message.split())\n",
    "            return new_message\n",
    "\n",
    "        def raw(text):\n",
    "            # print(\"Input text:\", text)  # Add this line for debugging\n",
    "            if 'introduction' in text:\n",
    "                new = text.split('introduction')\n",
    "                # print(\"Split result:\", new)  # Add this line for debugging\n",
    "                return new[1] \n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "\n",
    "        def header(text):\n",
    "            if 'introduction' in text:\n",
    "                new = text.split('introduction')\n",
    "                print(new[0])\n",
    "                return new[0]   \n",
    "            else:\n",
    "                return ''\n",
    "            \n",
    "        def abstract(text):\n",
    "            new = text.split('abstract')\n",
    "            if len(new) > 1:\n",
    "                return new[1]\n",
    "            else:\n",
    "                return ''  \n",
    "            \n",
    "            \n",
    "\n",
    "        def pc(data_text):\n",
    "            data_text = data_text.split('\\\"' and \",\" and \":\" and '\\”' and '\\“')\n",
    "            text = \"\"\n",
    "            for i in data_text:\n",
    "                text = text + i\n",
    "            print(len(text))\n",
    "\n",
    "            text = \"\"\n",
    "\n",
    "            for i in data_text:\n",
    "                text = text + i\n",
    "    \n",
    "            return text\n",
    "        \n",
    "        mail_pattern = re.compile(r'\\b\\S+@\\S+\\.\\S+\\.?\\S*\\b')\n",
    "        def remove_mail(text):\n",
    "            return mail_pattern.sub('', text)\n",
    "        \n",
    "        url_pattern = r'https?://(?:www\\.)?[^ ]+|www[^ ]+'\n",
    "\n",
    "        def add_urls(text):\n",
    "            httplinks = re.findall(url_pattern,text)\n",
    "            return httplinks\n",
    "\n",
    "        url_pattern = re.compile(url_pattern)\n",
    "        \n",
    "        def remove_urls(text):\n",
    "            return url_pattern.sub('', text)\n",
    "        \n",
    "        def ref(text):\n",
    "            if('references' in text):\n",
    "                terms = text.split('references')\n",
    "                return terms[1]\n",
    "            else:\n",
    "                return ''\n",
    "        def wref(text):\n",
    "            if('references' in text):\n",
    "                terms = text.split('references')\n",
    "                return terms[0]\n",
    "            else:\n",
    "                return text\n",
    "            \n",
    "        number_pattern = re.compile(r'\\b(?<!\\[)(-?\\d+(?:\\.\\d+)?)\\b(?![^\\[]*\\])')\n",
    "\n",
    "        def remove_number(text):\n",
    "            return number_pattern.sub('', text)\n",
    "\n",
    "\n",
    "        def sen_seg(data_text):\n",
    "            sentences = []\n",
    "\n",
    "            sentence = ''\n",
    "\n",
    "            # Iterate through the text\n",
    "            for i in range(len(data_text)):\n",
    "                char = data_text[i]\n",
    "                # Append character to the current sentence\n",
    "                sentence += char\n",
    "                if char == '.':\n",
    "                    # Check if the character before and after '.' are both numbers\n",
    "                    if i > 0 and i < len(data_text) - 1 and data_text[i-1].isdigit() and data_text[i+1].isdigit():\n",
    "                        continue  # Skip adding '.' to the sentence\n",
    "                    else:\n",
    "                        sentences.append(sentence.strip())\n",
    "                        sentence = ''\n",
    "\n",
    "            return sentences\n",
    "        \n",
    "        def checker(sentences):\n",
    "            new = []\n",
    "            for s in sentences:\n",
    "                if(len(s) > 30):\n",
    "                    new.append(s)\n",
    "            return new\n",
    "\n",
    "        \n",
    "        def merge_sen(sentences):\n",
    "            separator = \" \"\n",
    "            clean = separator.join(sentences)\n",
    "            return clean\n",
    "\n",
    "        df['Title'] = df['Title'].apply(split_message)\n",
    "\n",
    "        # print(df['raw_text'][0])\n",
    "        df['header'] = df['raw_text'].apply(header)\n",
    "        print(df['header'][0])\n",
    "        df['abstract'] = df['header'].apply(abstract)\n",
    "        df['raw_text'] = df['raw_text'].apply(raw)\n",
    "        df['pc_removal'] = df['raw_text'].apply(pc)\n",
    "        df['wmail'] = df['pc_removal'].apply(remove_mail)\n",
    "        df['urls'] = df['wmail'].apply(add_urls)\n",
    "        df['wurl'] = df['wmail'].apply(remove_urls)\n",
    "        df['refs'] = df['wurl'].apply(ref)\n",
    "        df['wrefs'] = df['wurl'].apply(wref)\n",
    "        df['wnumbers'] = df['wrefs']\n",
    "        df['sentences'] = df['wnumbers'].apply(sen_seg)\n",
    "        df['sentences'] = df['sentences'].apply(checker)\n",
    "        df['clean'] = df['sentences'].apply(merge_sen)\n",
    "   \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    \n",
    "    combined_df = pd.DataFrame() \n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                df = process_pdf(pdf_path) \n",
    "                # print(df['clean'][0])\n",
    "                combined_df = pd.concat([combined_df, df])\n",
    "\n",
    "        for dir in dirs:\n",
    "            subdir = os.path.join(root, dir)\n",
    "            process_directory(subdir)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "pdf_directory = './../Createdb/docs'\n",
    "\n",
    "if os.path.isdir(pdf_directory):\n",
    "    combined_df = process_directory(pdf_directory)\n",
    "    # Check if the combined DataFrame is not empty\n",
    "    if not combined_df.empty:\n",
    "        # Export the combined DataFrame to CSV\n",
    "        combined_df.to_csv(\"combined_data.csv\", index=False)\n",
    "    else:\n",
    "        print(\"Combined DataFrame is empty.\")\n",
    "else:\n",
    "    if pdf_directory.endswith('.pdf'):\n",
    "        df = process_pdf(pdf_directory)  \n",
    "        # Check if the DataFrame is not empty\n",
    "        if not df.empty:\n",
    "            # Export the DataFrame to CSV\n",
    "            df.to_csv(\"combined_data.csv\", index=False)\n",
    "            \n",
    "        else:\n",
    "            print(\"DataFrame is empty.\")\n",
    "    else:\n",
    "        print(\"Invalid path. Please provide a directory or a PDF file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>Title</th>\n",
       "      <th>header</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pc_removal</th>\n",
       "      <th>wmail</th>\n",
       "      <th>urls</th>\n",
       "      <th>wurl</th>\n",
       "      <th>refs</th>\n",
       "      <th>wrefs</th>\n",
       "      <th>wnumbers</th>\n",
       "      <th>sentences</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nthe subject of structural health monitoring ...</td>\n",
       "      <td>http : / / a m m t i a c . a l i o n s c i e n...</td>\n",
       "      <td>http://ammtiac.alionscience.com the ammtiac qu...</td>\n",
       "      <td></td>\n",
       "      <td>\\nthe subject of structural health monitoring ...</td>\n",
       "      <td>\\nthe subject of structural health monitoring ...</td>\n",
       "      <td>[http://ammtiac.alionscience.com/quarterly\\nht...</td>\n",
       "      <td>\\nthe subject of structural health monitoring ...</td>\n",
       "      <td>\\n[1] structural health monitoring workshop, s...</td>\n",
       "      <td>\\nthe subject of structural health monitoring ...</td>\n",
       "      <td>\\nthe subject of structural health monitoring ...</td>\n",
       "      <td>[the subject of structural health monitoring (...</td>\n",
       "      <td>the subject of structural health monitoring (s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nthe damage identification in composites lami...</td>\n",
       "      <td>Structuraldamageidentiﬁcationinlaminatedstruct...</td>\n",
       "      <td>structural damage identification in laminated ...</td>\n",
       "      <td>\\na damage identification technique based on f...</td>\n",
       "      <td>\\nthe damage identification in composites lami...</td>\n",
       "      <td>\\nthe damage identification in composites lami...</td>\n",
       "      <td>[www.elsevier.com/locate/compstruct\\nto]</td>\n",
       "      <td>\\nthe damage identification in composites lami...</td>\n",
       "      <td>\\n[1] doebling sw, farrar cr, prime mb, shevit...</td>\n",
       "      <td>\\nthe damage identification in composites lami...</td>\n",
       "      <td>\\nthe damage identification in composites lami...</td>\n",
       "      <td>[the damage identification in composites lamin...</td>\n",
       "      <td>the damage identification in composites lamina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nstructural optimization has become one of th...</td>\n",
       "      <td>JournalofConstructionalSteelResearch</td>\n",
       "      <td>journal of constructional steel research 65 (2...</td>\n",
       "      <td>\\nin this paper, a particle swarm optimizer wi...</td>\n",
       "      <td>\\nstructural optimization has become one of th...</td>\n",
       "      <td>\\nstructural optimization has become one of th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>\\nstructural optimization has become one of th...</td>\n",
       "      <td>\\n[1] kaveh a, farahmand azar b, talatahari s....</td>\n",
       "      <td>\\nstructural optimization has become one of th...</td>\n",
       "      <td>\\nstructural optimization has become one of th...</td>\n",
       "      <td>[structural optimization has become one of the...</td>\n",
       "      <td>structural optimization has become one of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_text  \\\n",
       "0  \\nthe subject of structural health monitoring ...   \n",
       "0  \\nthe damage identification in composites lami...   \n",
       "0  \\nstructural optimization has become one of th...   \n",
       "\n",
       "                                               Title  \\\n",
       "0  http : / / a m m t i a c . a l i o n s c i e n...   \n",
       "0  Structuraldamageidentiﬁcationinlaminatedstruct...   \n",
       "0               JournalofConstructionalSteelResearch   \n",
       "\n",
       "                                              header  \\\n",
       "0  http://ammtiac.alionscience.com the ammtiac qu...   \n",
       "0  structural damage identification in laminated ...   \n",
       "0  journal of constructional steel research 65 (2...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0                                                      \n",
       "0  \\na damage identification technique based on f...   \n",
       "0  \\nin this paper, a particle swarm optimizer wi...   \n",
       "\n",
       "                                          pc_removal  \\\n",
       "0  \\nthe subject of structural health monitoring ...   \n",
       "0  \\nthe damage identification in composites lami...   \n",
       "0  \\nstructural optimization has become one of th...   \n",
       "\n",
       "                                               wmail  \\\n",
       "0  \\nthe subject of structural health monitoring ...   \n",
       "0  \\nthe damage identification in composites lami...   \n",
       "0  \\nstructural optimization has become one of th...   \n",
       "\n",
       "                                                urls  \\\n",
       "0  [http://ammtiac.alionscience.com/quarterly\\nht...   \n",
       "0           [www.elsevier.com/locate/compstruct\\nto]   \n",
       "0                                                 []   \n",
       "\n",
       "                                                wurl  \\\n",
       "0  \\nthe subject of structural health monitoring ...   \n",
       "0  \\nthe damage identification in composites lami...   \n",
       "0  \\nstructural optimization has become one of th...   \n",
       "\n",
       "                                                refs  \\\n",
       "0  \\n[1] structural health monitoring workshop, s...   \n",
       "0  \\n[1] doebling sw, farrar cr, prime mb, shevit...   \n",
       "0  \\n[1] kaveh a, farahmand azar b, talatahari s....   \n",
       "\n",
       "                                               wrefs  \\\n",
       "0  \\nthe subject of structural health monitoring ...   \n",
       "0  \\nthe damage identification in composites lami...   \n",
       "0  \\nstructural optimization has become one of th...   \n",
       "\n",
       "                                            wnumbers  \\\n",
       "0  \\nthe subject of structural health monitoring ...   \n",
       "0  \\nthe damage identification in composites lami...   \n",
       "0  \\nstructural optimization has become one of th...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [the subject of structural health monitoring (...   \n",
       "0  [the damage identification in composites lamin...   \n",
       "0  [structural optimization has become one of the...   \n",
       "\n",
       "                                               clean  \n",
       "0  the subject of structural health monitoring (s...  \n",
       "0  the damage identification in composites lamina...  \n",
       "0  structural optimization has become one of the ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
